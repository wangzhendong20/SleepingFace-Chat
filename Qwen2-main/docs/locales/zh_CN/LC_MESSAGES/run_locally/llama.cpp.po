# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#

msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-06-06 19:37+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../source/run_locally/llama.cpp.rst:2 e7db733a92e84d9b8858cad10d251a75
msgid "llama.cpp"
msgstr "llama.cpp"

#: ../../source/run_locally/llama.cpp.rst:4 6b007bb8d4624ef5bc8278693ec39a41
msgid ""
"`llama.cpp <https://github.com/ggerganov/llama.cpp>`__ is a C++ library "
"for LLM inference with mimimal setup. It enables running Qwen on your "
"local machine. It is a plain C/C++ implementation without dependencies, "
"and it has AVX, AVX2 and AVX512 support for x86 architectures. It "
"provides 2, 3, 4, 5, 6, and 8-bit quantization for faster inference and "
"reduced memory footprint. CPU+GPU hybrid inference to partially "
"accelerate models larger than the total VRAM capacity is also supported. "
"Essentially, the usage of llama.cpp is to run the GGUF (GPT-Generated "
"Unified Format ) models. For more information, please refer to the "
"official GitHub repo. Here we demonstrate how to run Qwen with llama.cpp."
msgstr ""
"`llama.cpp <https://github.com/ggerganov/llama.cpp>`__ "
"是一个C++库，用于简化LLM推理的设置。它使得在本地机器上运行Qwen成为可能。该库是一个纯C/C++实现，不依赖任何外部库，并且针对x86架构提供了AVX、AVX2和AVX512加速支持。此外，它还提供了2、3、4、5、6以及8位量化功能，以加快推理速度并减少内存占用。对于大于总VRAM容量的大规模模型，该库还支持CPU+GPU混合推理模式进行部分加速。本质上，llama.cpp的用途在于运行GGUF（由GPT生成的统一格式）模型。欲了解更多详情，请参阅官方GitHub仓库。以下我们将演示如何使用llama.cpp运行Qwen。"

#: ../../source/run_locally/llama.cpp.rst:17 ea71da63c0a24583ab55e2b162d3f673
msgid "Prerequisites"
msgstr "准备"

#: ../../source/run_locally/llama.cpp.rst:19 a95ae6398fe0438eb8fd0c28a8832694
msgid ""
"This example is for the usage on Linux or MacOS. For the first step, "
"clone the repo and enter the directory:"
msgstr "这个示例适用于Linux或MacOS系统。第一步操作是： “克隆仓库并进入该目录："

#: ../../source/run_locally/llama.cpp.rst:27 625c94d0577147cdafcf58b04b575dcd
msgid "Then use ``make``:"
msgstr "然后运行 ``make`` 命令："

#: ../../source/run_locally/llama.cpp.rst:33 38462357b4d14ca1a75d114dac692600
msgid "Then you can run GGUF files with ``llama.cpp``."
msgstr "然后你就能使用 ``llama.cpp`` 运行GGUF文件。"

#: ../../source/run_locally/llama.cpp.rst:36 974a58bcb19f4d0cb9ee9e9215f4b166
msgid "Running Qwen GGUF Files"
msgstr "运行Qwen的GGUF文件"

#: ../../source/run_locally/llama.cpp.rst:38 8d8531d3723e45329b4d184e1bfbd2aa
msgid ""
"We provide a series of GGUF models in our Hugging Face organization, and "
"to search for what you need you can search the repo names with ``-GGUF``."
" Download the GGUF model that you want with ``huggingface-cli`` (you need"
" to install it first with ``pip install huggingface_hub``):"
msgstr ""
"我们在Hugging Face组织中提供了一系列GGUF模型，为了找到您需要的模型，您可以搜索仓库名称中包含 ``-GGUF`` "
"的部分。要下载所需的GGUF模型，请使用  ``huggingface-cli`` （首先需要通过命令 ``pip install "
"huggingface_hub`` 安装它）："

#: ../../source/run_locally/llama.cpp.rst:48 6885ad4a54f242ddb062e1a5b784bf18
msgid "for example:"
msgstr "比如："

#: ../../source/run_locally/llama.cpp.rst:54 0816836ed674460a8f4e2081eb53d7a9
msgid "Then you can run the model with the following command:"
msgstr "然后你可以用如下命令运行模型："

#: ../../source/run_locally/llama.cpp.rst:60 fb09c6e75f5b4f03b09c1cc37656d3ec
msgid ""
"where ``-n`` refers to the maximum number of tokens to generate. There "
"are other hyperparameters for you to choose and you can run"
msgstr "``-n`` 指的是要生成的最大token数量。这里还有其他超参数供你选择，并且你可以运行"

#: ../../source/run_locally/llama.cpp.rst:67 17d7934d8f91441392a39d74a1ca9ff9
msgid "to figure them out."
msgstr "以了解它们。"

#: ../../source/run_locally/llama.cpp.rst:70 f4e62da44014487aa554a054473dddb2
msgid "Make Your GGUF Files"
msgstr "生成你的GGUF文件"

#: ../../source/run_locally/llama.cpp.rst:72 caa62ed5d13d417091ef80dd59cffd92

msgid ""
"We introduce the method of creating and quantizing GGUF files in "
"`quantization/llama.cpp <../quantization/gguf.html>`__. You can refer to "
"that document for more information."
msgstr ""
"我们在 `quantization/llama.cpp <../quantization/gguf.html>`__ "
"中介绍了创建和量化GGUF文件的方法。您可以参考该文档获取更多信息。"

#: ../../source/run_locally/llama.cpp.rst:77 2a28e87e3cdf4ed49be8c2ef3b3a7483
msgid "Perplexity Evaluation"
msgstr "PPL评测"

#: ../../source/run_locally/llama.cpp.rst:79 ac709adde51c4fa68f917a409fc0a97b
msgid ""
"``llama.cpp`` provides methods for us to evaluate the perplexity "
"performance of the GGUF models. To do this, you need to prepare the "
"dataset, say \"wiki test\". Here we demonstrate an example to run the "
"test."
msgstr "llama.cpp为我们提供了评估GGUF模型PPL性能的方法。为了实现这一点，你需要准备一个数据集，比如“wiki测试”。这里我们展示了一个运行测试的例子。"

#: ../../source/run_locally/llama.cpp.rst:84 64e20019e6d549fe87556f5c0b6ffeeb
msgid "For the first step, download the dataset:"
msgstr "第一步，下载数据集："

#: ../../source/run_locally/llama.cpp.rst:91 62b3bac316bc4bbc8a72b40d9888ce51
msgid "Then you can run the test with the following command:"
msgstr "然后你可以用如下命令运行测试："

#: ../../source/run_locally/llama.cpp.rst:97 c014dfb5f69341cbbf4e595eef18d496
msgid "where the output is like"
msgstr "输出如下所示"

#: ../../source/run_locally/llama.cpp.rst:105 3443cfd163d84c82a75bbd03c1641727
msgid "Wait for some time and you will get the perplexity of the model."
msgstr "稍等一段时间你将得到模型的PPL评测结果。"

#: ../../source/run_locally/llama.cpp.rst:108 7e0280a7be3e4a5eb0fd4f11f242cd6a
msgid "Use GGUF with LM Studio"
msgstr "在LM Studio使用GGUF"

#: ../../source/run_locally/llama.cpp.rst:110 25b52c151477480ca42901dcb84c3c5d

msgid ""
"If you still find it difficult to use ``llama.cpp``, I advise you to play"
" with `LM Studio <https://lmstudio.ai/>`__, which is a platform for your "
"to search and run local LLMs. Qwen2 has already been officially part of "
"LM Studio. Have fun!"
msgstr ""
"如果你仍然觉得使用llama.cpp有困难，我建议你尝试一下 `LM Studio <https://lmstudio.ai/>`__ "
"这个平台，它允许你搜索和运行本地的大规模语言模型。Qwen2已经正式成为LM Studio的一部分。祝你使用愉快！"

